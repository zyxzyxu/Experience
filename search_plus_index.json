{"./":{"url":"./","title":"Introduction","keywords":"","body":"Study_Notes 日常总结的学习笔记 github：https://github.com/zyxzyxu email：zyxzyxu@163.com 使用知识共享 署名-相同方式共享 4.0协议发布            updated 2022-06-22 14:56:54 "},"HBase/HBase进阶.html":{"url":"HBase/HBase进阶.html","title":"HBase进阶","keywords":"","body":"1. HBase进阶1. HBase进阶 Master架构 Master 主要进程，具体实现类为HMaster，通常部署在namenode上 通过zookeeper管理分布在各个服务器上的 regionServer Master服务端 负载均衡器 通过读取meta表了解Regin的分配，通过连接zk了解RS的启动情况。5分钟调控一次分配平衡 元数据表管理器 (hdfs文件系统：元数据表->/hbase/data/hbase/meta) 管理Master自己的预写日志，如果宕机，让backUpMaster读取日志数据 MasterProcWAL预写日志管理器 (hdfs文件系统：MasterWAL数据->/hbase/MasterData/WALs) 本质写数据到hdfs，32M文件或者1H滚动当操作执行到meta表之后删除WAL Region Server架构 Region Server 主要进程，具体实现类为HRegionServer，通常部署在dataNode上。 Region Server服务端 WAL hdfs文件系统的WAL预写日志 Block cache 读缓存（数量1） 当Client调用get请求时，会从hdfs文件系统存储的Table表中读取每个RS管理的Region信息，存一份到内存中方便下次读取 Mem Store 写缓存（数量 = store个数） 当Client调用put请求时，会先写入内存中的Mem Store中，攒的过程中不断按照Row_key进行排序，攒够一批写入hdfs管理的table表中 除了主要的组件，还会启动多个线程监控一些必要的服务： Region拆分 Region合并 MemStore刷写 WAL预写日志滚动 写流程 HBase写流程 Client先问zookeeper请求创建连接,获取meta表位于哪个Region Server。 访问对应的Region Server读取meta表，查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的MetaCache，方便下次访问。 与目标RegionServer进行通讯； 将数据顺序写入（追加）到WAL； 将数据写入对应的Mem Store，数据会在Mem Story进行排序； 向客户端发送ack； 等达到Mem Store的刷写时机后，将数据刷写到HFile。 MemStore Flush MemStore刷写时机： 当某个memstroe的大小达到了 hbase.hregion.memstore.flush.size（默认值128M），其所在region的所有memstore都会刷写。 当memstore的大小达到了hbase.hregion.memstore.flush.size（默认值128M）* hbase.hregion.memstore.block.multiplier（默认值4）时，会阻止继续往该memstore写数据。 当region server中memstore的总大小达到java_heapsize hbase.regionserver.global.memstore.size（默认值0.4） hbase.regionserver.global.memstore.size.lower.limit（默认值0.95），region会按照其所有memstore的大小顺序（由大到小）依次进行刷写。直到region server中所有memstore的总大小减小到上述值以下。当region server中memstore的总大小达到java_heapsize* hbase.regionserver.global.memstore.size（默认值0.4）时，会阻止继续往所有的memstore写数据。 到达自动刷写的时间，也会触发memstore flush。自动刷新的时间间隔由该属性进行配置hbase.regionserver.optionalcacheflushinterval（默认1小时）。 当WAL文件的数量超过hbase.regionserver.max.logs，region会按照时间顺序依次进行刷写，直到WAL文件数量减小到hbase.regionserver.max.log以下（该属性名已经废弃，现无需手动设置，最大值为32）。 读流程 HBase读流程 Client先访问zookeeper，获取hbase:meta表位于哪个Region Server。 访问对应的Region Server，获取hbase:meta表,查询出目标数据位于哪个Region Server中的哪个Region中。并将该table的region信息以及meta表的位置信息缓存在客户端的meta cache，方便下次访问。 与目标Region Server进行通讯； 分别在BlockCache和MemStore和Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。 将查询到的新的数据块（Block，HFile数据存储单元，默认大小为64KB）缓存到Block Cache。 将合并后的最终结果返回给客户端。 HFile 合并（默认是7天） Compaction分为两种，分别是Minor Compaction和Major Compaction Minor Compaction会将临近的若干个较小的HFile合并成一个较大的HFile，并清理掉部分过期和删除的数据。 Major Compaction会将一个Store下的所有的HFile合并成一个大HFile，并且会清理掉所有过期和删除的数据。 HBase优化 预分区 手动设定预分区create 'staff1','info', SPLITS => ['1000','2000','3000','4000'] 生成16进制序列预分区create 'staff2','info',{NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'} 按照文件中设置的规则预分区create 'staff3', 'info',SPLITS_FILE => 'splits.txt' 使用JavaAPI创建预分区 RowKey设计 生成随机数、hash、散列值 字符串反转 字符串拼接 基础优化 Zookeeper会话超时时间hbase-site.xml 属性：zookeeper.session.timeout 解释：默认值为90000毫秒（90s）。当某个RegionServer挂掉，90s之后Master才能察觉到。可适当减小此值，以加快Master响应，可调整至60000毫秒。 设置RPC监听数量hbase-site.xml 属性：hbase.regionserver.handler.count 解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。 手动控制Major Compactionhbase-site.xml 属性：hbase.hregion.majorcompaction 解释：默认值：604800000秒（7天）， Major Compaction的周期，若关闭自动Major Compaction，可将其设为0 优化HStore文件大小hbase-site.xml 属性：hbase.hregion.max.filesize 解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。 优化HBase客户端缓存hbase-site.xml 属性：hbase.client.write.buffer 解释：默认值2097152bytes（2M）用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小， 以达到减少RPC次数的目的。 指定scan.next扫描HBase所获取的行数hbase-site.xml 属性：hbase.client.scanner.caching 解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。 BlockCache占用RegionServer堆内存的比例hbase-site.xml 属性：hfile.block.cache.size 解释：默认0.4，读请求比较多的情况下，可适当调大 MemStore占用RegionServer堆内存的比例hbase-site.xml 属性：hbase.regionserver.global.memstore.size 解释：默认0.4，写请求较多的情况下，可适当调大 使用知识共享 署名-相同方式共享 4.0协议发布            updated 2022-06-22 14:56:54 "},"Hive/Hive参数与性能调优.html":{"url":"Hive/Hive参数与性能调优.html","title":"Hive参数与性能调优","keywords":"","body":"1. Hive性能调优的方式1. Hive性能调优的方式 SQL语句优化 union all insert into table test01 partition(tp) select age，max(birthday) birthday，'max' tp from user group by age union all insert into table test01 partition(tp) select age，min(birthday) birthday，'min' tp from user group by age; union all 前后的两个语句都是对同一张表按照age进行分组，然后分别取最大、最小值，对同一张表相同的字段进行两次分组，可以进行优化，使用语法：from ... insert into ... 使用一张表，可以进行多次插入操作： --开启动态分区 set hive.exec.dynamic.partition=true； set hive.exec.dynamic.partition.mode=nonstrict; from user insert into table test01 partition(tp) select age，max(birthday) birthday，'max' tp group by age insert into table stu partition(tp) select age，min(birthday) birthday，'min' tp group by age; distinct select count(1) from (select age from test01 group by age) a; select count(distinct age) from test01; 在数据量特别大的情况下使用第一种方式可以有效避免Reduce端的数据倾斜 就当前的业务和环境下使用distinct一定会比上面那种子查询的方式效率高，原因如下： 在当前案例下，去重字段为年龄，年龄的枚举值非常有限，age的最大枚举值才是100，如果转化成MapReduce来解释的话，在Map阶段，每个Map会对age去重。由于age枚举值有限，因而每个Map得到的age也有限，最终得到reduce的数据量也就是map数量×age枚举值的个数 最新的Hive 3.0中新增了 count(distinct)优化，通过配置hive.optimize.countdistinct，即使真的出现数据倾斜也可以自动优化，自动改变SQL执行的逻辑 第二种方式比第一种简洁明了，如果没有特殊问题，简洁为优 在一些情况下不要过度优化，调优讲究适时调优，过早调优可能做的是无用功甚至负效应 数据格式优化 Hive提供多种数据存储组织格式，不同格式对程序的运行效率也会有极大的影响 Hive提供的格式有TEXT、SequenceFile、RCFile、ORC和Parquet等 SequenceFile是一个二进制key/value对结构的平面文件，在早期的Hadoop平台上被广泛用于MapReduce输出/输出格式，以及作为数据存储格式 Parquet是一种列式数据存储格式，可以兼容多种计算引擎，如MapRedcue和Spark等，对多层嵌套的数据结构提供了良好的性能支持，是目前Hive生产环境中数据存储的主流选择之一 ORC优化是对RCFile的一种优化，它提供了一种高效的方式来存储Hive数据，同时也能够提高Hive的读取、写入和处理数据的性能，能够兼容多种计算引擎。事实上，在实际的生产环境中，ORC已经成为了Hive在数据存储上的主流选择之一 数据格式 CPU时间 用户等待耗时 TextFile 33分 171秒 SequenceFile 38分 162秒 Parquet 2分22秒 50秒 ORC 1分52秒 56秒 CPU时间：表示运行程序所占用服务器CPU资源的时间 用户等待耗时：记录的是用户从提交作业到返回结果期间用户等待的所有时间 小文件过多优化 小文件如果过多，对 hive 来说，在进行查询时，每个小文件都会当成一个块，启动一个Map任务来完成，而一个Map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的Map数量是受限的 小文件产生原因 直接向表中插入数据insert into table test02 values (1，'a')，(2，'b'); 多次插入少量数据就会出现多个小文件，生产环境基本没有使用 通过load方式加载数据#导入文件 load data local inpath '/export/list.csv' overwrite into table test02 #导入文件夹 load data local inpath '/export/list' overwrite into table test02 使用load data方式可以导入文件或文件夹，当导入一个文件时，hive表就有一个文件，当导入文件夹时，hive表的文件数量为文件夹下所有文件的数量 通过查询方式加载数据insert overwrite table test02 select id，name from test03; 这种方式是生产环境中常用的，也是最容易产生小文件的方式 insert 导入数据时会启动 MR 任务，MR 中 reduce 有多少个就输出多少个文件，所以，文件数量 = ReduceTask数量×分区数 也有很多简单任务没有reduce，只有map阶段，则文件数量 = MapTask数量×分区数 每执行一次 insert 时hive中至少产生一个文件，因为 insert 导入时至少会有一个MapTask。 像有的业务需要每10分钟就要把数据同步到 hive 中，这样产生的文件就会很多。 小文件过多产生的影响 首先对底层存储HDFS来说，HDFS本身就不适合存储大量小文件，小文件过多会导致namenode元数据特别大，占用太多内存，严重影响HDFS的性能 对 hive 来说，在进行查询时，每个小文件都会当成一个块，启动一个Map任务来完成，而一个Map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的Map数量是受限的。 解决小文件过多 使用 hive 自带的 concatenate 命令，自动合并小文件 使用方法： #对于非分区表 alter table A concatenate; #对于分区表 alter table B partition(day=20220101) concatenate; 举例： #向 A 表中插入数据 hive (default)> insert into table A values (1,'aa',67),(2,'bb',87); hive (default)> insert into table A values (3,'cc',67),(4,'dd',87); hive (default)> insert into table A values (5,'ee',67),(6,'ff',87); #执行以上三条语句，则A表下就会有三个小文件,在hive命令行执行如下语句 #查看A表下文件数量 hive (default)> dfs -ls /user/hive/warehouse/A; Found 3 items -rwxr-xr-x 3 root supergroup 378 2022-01-01 14：46 /user/hive/warehouse/A/000000_0 -rwxr-xr-x 3 root supergroup 378 2022-01-01 14：47 /user/hive/warehouse/A/000000_0_copy_1 -rwxr-xr-x 3 root supergroup 378 2022-01-01 14：48 /user/hive/warehouse/A/000000_0_copy_2 #可以看到有三个小文件，然后使用 concatenate 进行合并 hive (default)> alter table A concatenate; #再次查看A表下文件数量 hive (default)> dfs -ls /user/hive/warehouse/A; Found 1 items -rwxr-xr-x 3 root supergroup 778 2020-12-24 14：59 /user/hive/warehouse/A/000000_0 #已合并成一个文件 注意： 1、concatenate 命令只支持 RCFILE 和 ORC 文件类型。 2、使用concatenate命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令。 3、当多次使用concatenate后文件数量不在变化，这个跟参数 mapreduce.input.fileinputformat.split.minsize=256mb 的设置有关，可设定每个文件的最小size。 调整参数减少Map数量 设置map输入合并小文件的相关参数： #执行Map前进行小文件合并 #CombineHiveInputFormat底层是 Hadoop的 CombineFileInputFormat 方法 #此方法是在mapper中将多个文件合成一个split作为输入 set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; -- 默认 #每个Map最大输入大小(这个值决定了合并后文件的数量) set mapred.max.split.size=256000000; -- 256M #一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并) set mapred.min.split.size.per.node=100000000; -- 100M #一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并) set mapred.min.split.size.per.rack=100000000; -- 100M 设置map输出和reduce输出进行合并的相关参数： #设置map端输出进行合并，默认为true set hive.merge.mapfiles = true; #设置reduce端输出进行合并，默认为false set hive.merge.mapredfiles = true; #设置合并文件的大小 set hive.merge.size.per.task = 256*1000*1000; -- 256M #当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge set hive.merge.smallfiles.avgsize=16000000; -- 16M 启用压缩： # hive的查询结果输出是否进行压缩 set hive.exec.compress.output=true; # MapReduce Job的结果输出是否使用压缩 set mapreduce.output.fileoutputformat.compress=true; 减少Reduce的数量 #reduce 的个数决定了输出的文件的个数，所以可以调整reduce的个数控制hive表的文件数量， #hive中的分区函数 distribute by 正好是控制MR中partition分区的， #然后通过设置reduce的数量，结合分区函数让数据均衡的进入每个reduce即可。 #设置reduce的数量有两种方式，第一种是直接设置reduce个数 set mapreduce.job.reduces=10; #第二种是设置每个reduce的大小，Hive会根据数据总大小猜测确定一个reduce个数 set hive.exec.reducers.bytes.per.reducer=5120000000; -- 默认是1G，设置为5G #执行以下语句，将数据均衡的分配到reduce中 set mapreduce.job.reduces=10; insert overwrite table A partition(dt) select * from B distribute by rand(); #解释：如设置reduce数量为10，则使用 rand()， 随机生成一个数 x % 10 ， #这样数据就会随机进入 reduce 中，防止出现有的文件过大或过小 使用hadoop的archive将小文件归档 Hadoop Archive简称HAR，是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样在减少namenode内存使用的同时，仍然允许对文件进行透明的访问 #用来控制归档是否可用 set hive.archive.enabled=true; #通知Hive在创建归档时是否可以设置父目录 set hive.archive.har.parentdir.settable=true; #控制需要归档文件的大小 set har.partfile.size=1099511627776; #使用以下命令进行归档 ALTER TABLE A ARCHIVE PARTITION(dt='2022-01-01', hr='12'); #对已归档的分区恢复为原文件 ALTER TABLE A UNARCHIVE PARTITION(dt='2022-01-01', hr='12'); 注意：归档的分区可以查看不能 insert overwrite，必须先 unarchive。如果是新集群，没有历史遗留问题的话，建议hive使用 orc 文件格式，以及启用 lzo 压缩。这样小文件过多可以使用hive自带命令 concatenate 快速合并。 并行执行优化 Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可使得整个job的执行时间缩短。如果有更多的阶段可以并行执行，那么job可能就越快的完成。 通过设置参数hive.exec.parallel值为true，就可以开启并发执行。在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。 set hive.exec.parallel=true; //打开任务并行执行 set hive.exec.parallel.thread.number=16; //同一个sql允许最大并行度，默认为8。 当然得是在系统资源比较空闲的时候才有优势，否则没资源，并行也起不来。 数据倾斜优化 数据倾斜的原理都知道，就是某一个或几个key占据了整个数据的90%，这样整个任务的效率都会被这个key的处理拖慢，同时也可能会因为相同的key会聚合到一起造成内存溢出。 常见的做法，通过参数调优： set hive.map.aggr=true; set hive.groupby.skewindata = ture; 当选项设定为true时，生成的查询计划有两个MapReduce任务。 在第一个MapReduce中，map的输出结果集合会随机分布到reduce中，每个reduce做部分聚合操作，并输出结果。 这样处理的结果是，相同的Group By Key有可能分发到不同的reduce中，从而达到负载均衡的目的； 第二个MapReduce任务再根据预处理的数据结果按照Group By Key分布到reduce中（这个过程可以保证相同的Group By Key分布到同一个reduce中），最后完成最终的聚合操作。 但是这个处理方案对于我们来说是个黑盒，无法把控。 通常采用： sample采样，获取哪些集中的key； 将集中的key按照一定规则添加随机数； 进行join，由于打散了，所以数据倾斜避免了； 在处理结果中对之前的添加的随机数进行切分，变成原始的数据。 Limit限制调整优化 一般情况下，Limit语句还是需要执行整个查询语句，然后再返回部分结果。 有一个配置属性可以开启，避免这种情况：对数据源进行抽样。 hive.limit.optimize.enable=true -- 开启对数据源进行采样的功能 hive.limit.row.max.size -- 设置最小的采样容量 hive.limit.optimize.limit.file -- 设置最大的采样样本数 缺点：有可能部分数据永远不会被处理到 JOIN优化 使用相同的连接键 当对3个或者更多个表进行join连接时，如果每个on子句都使用相同的连接键的话，那么只会产生一个MapReduce job。 尽量尽早地过滤数据 减少每个阶段的数据量，对于分区表要加分区，同时只选择需要使用到的字段。 尽量原子化操作 尽量避免一个SQL包含复杂逻辑，可以使用中间表来完成复杂的逻辑。 代码优化原则： 理透需求原则，这是优化的根本； 把握数据全链路原则，这是优化的脉络； 坚持代码的简洁原则，这让优化更加简单； 没有瓶颈时谈论优化，这是自寻烦恼。 使用知识共享 署名-相同方式共享 4.0协议发布            updated 2022-06-22 14:56:54 "},"Hive/Hive常见问题集.html":{"url":"Hive/Hive常见问题集.html","title":"Hive常见问题集","keywords":"","body":"1. Hive问题1. Hive问题 使用jdbc连接hive后调用配置参数：set hive.input.format = org.apache.hadoop.hive.ql.io.HiveInputFormat; 提示报错信息：Error while processing statement: Cannot modify hive.input.format at runtime. It is not in list of params that are allowed to be modified at runtime hive设置权限后，修改追加参数白名单设置 解决方案： 在hive-site.xml中增加如下配置： ```xml hive.security.authorization.sqlstd.confwhitelist mapred.*|hive.*|mapreduce.*|spark.* hive.security.authorization.sqlstd.confwhitelist.append mapred.|hive.|mapreduce.|spark. ``` 使用知识共享 署名-相同方式共享 4.0协议发布            updated 2022-06-22 14:56:54 "},"gitbook.html":{"url":"gitbook.html","title":"Gitbook","keywords":"","body":"GitBook 开启创作记录之旅 插件整理 book.json配置 使用知识共享 署名-相同方式共享 4.0协议发布            updated 2022-06-22 14:56:54 "}}